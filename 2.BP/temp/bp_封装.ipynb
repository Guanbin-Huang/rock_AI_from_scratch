{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import struct\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "np.random.seed(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def decode_labels(file):\n",
    "    '''\n",
    "    解码标签文件\n",
    "    '''\n",
    "    \n",
    "    # rb  ->  read binary\n",
    "    with open(file, \"rb\") as f:\n",
    "        binary_data = f.read()\n",
    "        \n",
    "    # 大端方式解析出2个int32，返回的是tuple(int1, int2)\n",
    "    # int1 -> magic number，用来验证数据是否是目标数据\n",
    "    _, num_items = struct.unpack_from(\">II\", binary_data, 0)\n",
    "    labels = struct.unpack_from(\"B\" * num_items, binary_data, 8)\n",
    "    return np.array(labels).reshape(-1, 1).astype(np.int32)\n",
    "\n",
    "def decode_images(file):\n",
    "    '''\n",
    "    解码图像数据\n",
    "    '''\n",
    "    \n",
    "    # rb  ->  read binary\n",
    "    with open(file, \"rb\") as f:\n",
    "        binary_data = f.read()\n",
    "        \n",
    "    # 大端方式解析出4个int32，返回的是tuple(magic number, num images, rows, cols)\n",
    "    _, num_images, rows, cols = struct.unpack_from(\">IIII\", binary_data, 0)\n",
    "    images = struct.unpack_from(\"B\" * num_images * rows * cols, binary_data, 16)\n",
    "    return np.array(images).reshape(-1, rows * cols)\n",
    "\n",
    "\n",
    "def one_hot(t, num_classes):   # 将标量转换为矩阵，方便计算\n",
    "    \n",
    "    rows = t.shape[0]\n",
    "    output = np.zeros((rows, num_classes))\n",
    "    for row in range(rows):\n",
    "        label = t[row, 0]\n",
    "        output[row, label] = 1\n",
    "    return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "# 数据的封装\n",
    "class Dataset:\n",
    "    # 动态的，那么Dataset是个基类，所有动态的继承自Dataset\n",
    "    # 需要实现什么接口？\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "      \n",
    "    \n",
    "class MNISTDataset(Dataset):\n",
    "    # 针对mnist数据的解析、加载、归一化\n",
    "    def __init__(self, image_file, label_file):\n",
    "        \n",
    "        self.num_classes = 10\n",
    "        self.images = decode_images(image_file)\n",
    "        self.labels = decode_labels(label_file)\n",
    "        self.images = (self.images / 255.0 - 0.5).astype(np.float32)\n",
    "        self.labels_one_hot = one_hot(self.labels, self.num_classes)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # 角色的职责\n",
    "        # 实现图像加载、归一化/标准化、onehot\n",
    "        # 为什么要返回one_hot，计算时，使用one_hot比较方便\n",
    "        # 为什么要返回label，因为做测试的时候，label比较方便\n",
    "        # pytorch里面，CELoss使用的不是one_hot。所以以后不需要返回one_hot\n",
    "        return self.images[index], self.labels[index], self.labels_one_hot[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "\n",
    "class DataLoaderIterator:\n",
    "    # 数据加载器的迭代器\n",
    "    # 职责：\n",
    "    # 负责一轮数据的打乱、封装操作\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        \n",
    "        # 这里有2中处理方法\n",
    "        # 1.向上取整\n",
    "        # 2.整除，向下取整，多余部分丢弃\n",
    "        # 这里考虑用2\n",
    "        self.num_batch_per_epoch = len(dataloader)\n",
    "        \n",
    "        # 定义指针记录当前batch的索引\n",
    "        self.batch_cursor = 0\n",
    "        \n",
    "        # 实现一轮数据的打乱和封装获取\n",
    "        # 与其打乱数据，不如打乱索引\n",
    "        self.indexs = list(range(len(dataloader.dataset)))\n",
    "        \n",
    "        # 如果需要随机打乱，条件控制由dataloader的shuffle决定\n",
    "        if dataloader.shuffle:\n",
    "            np.random.shuffle(self.indexs)\n",
    "        \n",
    "        \n",
    "    def __next__(self):\n",
    "        # 如果到了一轮的边界，即迭代结束，抛出异常\n",
    "        if self.batch_cursor >= self.num_batch_per_epoch:\n",
    "            # 如果到了边界，抛出StopIteration\n",
    "            raise StopIteration()\n",
    "            \n",
    "        # 职责：对数据进行封装\n",
    "        #    b1  image.shape = 784,     label.shape = 1,     label_onehot.shape = 10,\n",
    "        #    b2  image.shape = 784,     label.shape = 1,     label_onehot.shape = 10,\n",
    "        #    b3  image.shape = 784,     label.shape = 1,     label_onehot.shape = 10,\n",
    "        #    \n",
    "        # images.shape = 3x784 = np.vstack()    labels.shape = 3x1,  one_hot.shape = 3x10\n",
    "        # 循环batch_size次。这里不用考虑边界问题。因为使用的是策略2，向下取整\n",
    "        # batch_data = [\n",
    "        #   [image1, image2, image3, ..., imagen],\n",
    "        #   [label1, label2, label3, ..., labeln],\n",
    "        #   [one_hot1, one_hot2, one_hot3, ..., one_hotn]\n",
    "        # ]\n",
    "        \n",
    "        batch_data = []\n",
    "        for i in range(self.dataloader.batch_size):\n",
    "            \n",
    "            # 拿到图像的索引，这个索引可能是打乱过的\n",
    "            # 10100个图，batch_size = 1000\n",
    "            # 一轮需要多少个batch，10个batch\n",
    "            # batch_cursor -> 0, 1, 2 ... 9\n",
    "            # batch_cursor = 5\n",
    "            # for i in range(batch_size):\n",
    "            #     基于10100的索引 = batch_cursor * batch_size + i\n",
    "            index = self.indexs[self.batch_cursor * self.dataloader.batch_size + i]\n",
    "            \n",
    "            # 从dataset中拿到数据，这个数据可能包含图像也可能包含标签\n",
    "            # 这里只关心有几个数据，并不关心数据是什么样\n",
    "            # 比如这里返回的是(image, label, label_onehot)\n",
    "            data_item = self.dataloader.dataset[index]\n",
    "            \n",
    "            if len(batch_data) == 0:\n",
    "                # 对batch_data做初始化\n",
    "                batch_data = [[] for _ in data_item]\n",
    "            \n",
    "            # 把data_item中的每一项，分门别类的放到batch_data中\n",
    "            for index, item in enumerate(data_item):\n",
    "                batch_data[index].append(item)\n",
    "                \n",
    "        self.batch_cursor += 1\n",
    "        \n",
    "        # 当整个batch的数据准备好过后，可以用np.vstack拼接在一起\n",
    "        for index in range(len(batch_data)):\n",
    "            batch_data[index] = np.vstack(batch_data[index])\n",
    "        return tuple(batch_data)\n",
    "    \n",
    "\n",
    "class DataLoader:\n",
    "    # 职责\n",
    "    # 实例化的时候需要指定dataset，batch_size，shuffle\n",
    "    # 数据的封装，打包为一个batch\n",
    "    # 对数据进行打乱\n",
    "    # 可以通过迭代器来获取一批一批的数据\n",
    "    #    好处是：使用一批创建一批\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # 实例化一个迭代器对象，将自身作为参数传入进去\n",
    "        return DataLoaderIterator(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        #用以告诉外界，多少次迭代，就算是完成一轮\n",
    "        # 这里有2种处理方法\n",
    "        # 1.向上取整\n",
    "        # 2.整除，向下取整，多余部分丢弃\n",
    "        # 这里考虑用2\n",
    "        return len(self.dataset) // self.batch_size\n",
    "\n",
    "# 计算的封装\n",
    "# 对参数的封装\n",
    "class Parameter:\n",
    "    # 实例化的时候，传入参数值\n",
    "    # 封装data、和grad，储存参数值和梯度\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.grad = np.zeros_like(data)\n",
    "        \n",
    "    # 清空参数中储存的梯度\n",
    "    def zero_grad(self):\n",
    "        self.grad[...] = 0\n",
    "    \n",
    "# 对layer的封装\n",
    "class Module:\n",
    "    # 1.可以称之为算子，那么他应该有forward、backward。为了简化代码，可以用__call__实现forward\n",
    "    # 2.需要实现以个params函数，拿出当前module下的所有【递归，如果有子类里面还有子类包含了参数，也要拿出来】参数实例\n",
    "    # 3.考虑有些算子，需要感知当前的环境属于训练还是推理，用training储存是否为训练状态提供给特定算子用。通过\n",
    "    # train方法和eval方法修改training的值\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, *args):\n",
    "        # forward输入参数可以是多个\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        # 假设算子输出只有一个，所以对应的梯度也应该只有一个\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        # 用来简化xxx.forward()的调用，改成xxx()\n",
    "        # 后期可以在__call__中，forward的前后加入一些代码，实现钩子编程范式\n",
    "        return self.forward(*args)\n",
    "    \n",
    "    def modules(self):\n",
    "        # 获取自身当前所有是Module的属性，并返回，这里没有递归。仅仅是自身\n",
    "        ms = []\n",
    "        for attr in self.__dict__:\n",
    "            m = self.__dict__[attr]\n",
    "            if isinstance(m, Module):\n",
    "                ms.append(m)\n",
    "        return ms\n",
    "        \n",
    "    def params(self):\n",
    "        # 获取当前模块下的所有参数，注意是递归获取\n",
    "        # 获取自身下的所有Parameter的属性\n",
    "        ps = []\n",
    "        for attr in self.__dict__:\n",
    "            p = self.__dict__[attr]\n",
    "            if isinstance(p, Parameter):\n",
    "                ps.append(p)\n",
    "                \n",
    "        # 获取自身下的所有Module，然后调用Module的params获取到参数并合并到ps中\n",
    "        # 有一个方法，获取所有当前类的module属性。这里不存在递归\n",
    "        ms = self.modules()\n",
    "        for m in ms:\n",
    "            # 这里是递归在调用\n",
    "            ps.extend(m.params())\n",
    "        return ps\n",
    "    \n",
    "    def train(self):\n",
    "        # 进入training模式，给到后续需要的算子必要信息\n",
    "        self.training = True\n",
    "        for m in self.modules():\n",
    "            # 这里再递归调用\n",
    "            m.train()\n",
    "            \n",
    "        # 返回self，是为了使用者可以链式调用\n",
    "        # a.train()\n",
    "        #    .eval()\n",
    "        #    .to(\"cuda\")\n",
    "        return self\n",
    "    \n",
    "    def eval(self):\n",
    "        # 进入评估模式，给到后续需要的算子必要信息\n",
    "        self.training = False\n",
    "        for m in self.modules():\n",
    "            # 这里再递归调用\n",
    "            m.eval()\n",
    "        return self\n",
    "    \n",
    "# A-\n",
    "#    - a Parameter\n",
    "#    - b Parameter\n",
    "#    - c Module\n",
    "#       - c1 Parameter\n",
    "#       - c2 Module\n",
    "#       - \n",
    "#    - d Module\n",
    "    \n",
    "    \n",
    "class Linear(Module):\n",
    "    # 线性算子，线性层\n",
    "    # 职责:\n",
    "    #  包含了参数（parameter），包含了运算过程（forward、backward），对于输入的梯度计算，\n",
    "    #  和对于参数（parameter）的梯度计算\n",
    "    def __init__(self, num_input, num_output):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 凯明初始化\n",
    "        # 1 / np.sqrt(num_input)\n",
    "        self.weight = Parameter(np.random.normal(0, 1, size=(num_input, num_output)))\n",
    "        self.bias   = Parameter(np.zeros((1, num_output)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 保存x给到backward时使用\n",
    "        self.x = x\n",
    "        \n",
    "        # 这里bias会发生广播，复制batch_size份bias进行加法操作\n",
    "        return x @ self.weight.data + self.bias.data\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        # grad.shape = forward(x).shape = batch_size x num_output\n",
    "        # bias.shape = 1 x num_output\n",
    "        # AB = C, deltaC = G\n",
    "        # deltaA = G @ B.T\n",
    "        # deltaB = A.T @ G\n",
    "        self.weight.grad += self.x.T @ grad\n",
    "        self.bias.grad   += np.sum(grad, axis=0, keepdims=True)\n",
    "        return grad @ self.weight.data.T\n",
    "\n",
    "    \n",
    "class Sigmoid(Module):\n",
    "    # 输入x，输出y。forward做sigmoid，backward求导\n",
    "    def forward(self, x):\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        return grad * self.y * (1 - self.y)\n",
    "    \n",
    "\n",
    "class Sequencial(Module):\n",
    "    def __init__(self, *items):\n",
    "        # 实现一堆模块的聚集，同时又能够被递归遍历到\n",
    "        super().__init__()\n",
    "        self.items = items\n",
    "        \n",
    "    def modules(self):\n",
    "        # 覆盖基类的modules方法，直接返回items即可\n",
    "        # 如果基类直接遍历__dict__属性，并判断类型是否为Module，此时items是tuple，不满足条件。所以得不到items\n",
    "        # 所以要覆盖基类方法\n",
    "        return self.items\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 按照顺序执行items即可\n",
    "        for m in self.items:\n",
    "            x = m(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        # 按照反向顺序，执行items中模块的backward\n",
    "        for item in self.items[::-1]:\n",
    "            grad = item.backward(grad)\n",
    "        return grad\n",
    "    \n",
    "\n",
    "class SoftmaxCrossEntropyLoss(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        ex = np.exp(x)\n",
    "        \n",
    "        # 坐标系为1，是在列方向求和（输出的特征维度上求和）\n",
    "        sumex = ex.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        # sumex.shape = batch_size x 1\n",
    "        # ex.shape   = batch_size x num_classes\n",
    "        self.probability = ex / sumex\n",
    "        self.batch_size = x.shape[0]\n",
    "        self.target = target\n",
    "        return -np.sum(self.target * np.log(self.probability)) / self.batch_size\n",
    "        \n",
    "    def backward(self, grad=1):\n",
    "        return grad * (self.probability - self.target) / self.batch_size\n",
    "    \n",
    "\n",
    "# 对模型的封装\n",
    "class Network(Module):\n",
    "    def __init__(self, num_feature, num_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.layers = Sequencial(\n",
    "            Linear(num_feature, num_hidden),\n",
    "            Sigmoid(),\n",
    "            Linear(num_hidden, num_classes)\n",
    "        )\n",
    "        self.loss = SoftmaxCrossEntropyLoss()\n",
    "        \n",
    "    def inference(self, x):\n",
    "        return self.layers(x)\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        return self.loss(self.inference(x), target)\n",
    "    \n",
    "    def backward(self, grad=1):\n",
    "        grad = self.loss.backward(grad)\n",
    "        return self.layers.backward(grad)\n",
    "\n",
    "    \n",
    "# 对优化器的封装\n",
    "# class Optimizer:\n",
    "#     def __init__(self, param_groups):\n",
    "#         # 给我所有的params，我给你做更新和应用\n",
    "#         self.param_groups = param_groups\n",
    "        \n",
    "#     def zero_grad(self):\n",
    "#         # 清空所有参数中的梯度\n",
    "#         # 如果需要累积梯度，可以自行控制\n",
    "#         for pg in self.param_groups:\n",
    "#             # pg is dict   {\"params\": network1.params(), \"lr\": 1e-3},\n",
    "#             for param in pg[\"params\"]:\n",
    "#                 param.zero_grad()\n",
    "            \n",
    "# Optimizer([\n",
    "#     {\"params\": network1.params(), \"lr\": 1e-3},\n",
    "#     {\"params\": network2.params(), \"lr\": 1e-2},\n",
    "# ])\n",
    "            \n",
    "    \n",
    "# 对优化器的封装\n",
    "class Optimizer:\n",
    "    def __init__(self, params, lr):\n",
    "        # 给我所有的params，我给你做更新和应用\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        # 清空所有参数中的梯度\n",
    "        # 如果需要累积梯度，可以自行控制\n",
    "        for param in self.params:\n",
    "            param.zero_grad()\n",
    "            \n",
    "    def set_lr(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "        \n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr=1e-1):\n",
    "        super().__init__(params, lr)\n",
    "        \n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.data -= self.lr * param.grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "batch_size = 256\n",
    "num_hidden = 100\n",
    "num_feature = 784\n",
    "num_epoch = 10\n",
    "\n",
    "train_dataset = MNISTDataset(\"../05.06.bp_optimizer/dataset/train-images-idx3-ubyte\", \"../05.06.bp_optimizer/dataset/train-labels-idx1-ubyte\")\n",
    "train_loader  = DataLoader(train_dataset, batch_size, True)\n",
    "test_dataset = MNISTDataset(\"../05.06.bp_optimizer/dataset/t10k-images-idx3-ubyte\", \"../05.06.bp_optimizer/dataset/t10k-labels-idx1-ubyte\")\n",
    "test_loader  = DataLoader(test_dataset, 512, True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "network = Network(num_feature, num_hidden, train_dataset.num_classes)\n",
    "optim   = SGD(network.params(), 0.1)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for images, labels, one_hots in train_loader:\n",
    "        loss = network(images, one_hots)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        network.backward()\n",
    "        optim.step()\n",
    "    print(f\"{loss:.3f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.633\n",
      "1.077\n",
      "1.035\n",
      "0.819\n",
      "0.701\n",
      "0.534\n",
      "0.634\n",
      "0.737\n",
      "0.563\n",
      "0.433\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}