{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import the related class and functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle as pkl\n",
    "import time\n",
    "import torch\n",
    "\n",
    "#-------------------------------------------------- 工具函数 ------------------------------------------------------\n",
    "#region\n",
    "def one_func_set_all_random_seed(seed=0):\n",
    "    # different random seeds\n",
    "    import torch\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "\n",
    "    import numpy as np\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    # for dataloader\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    return g\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    import random\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "_ = one_func_set_all_random_seed(0)\n",
    "\n",
    "\n",
    "def cal_time(func):\n",
    "    '''timer'''\n",
    "    def improved_func(*args):\n",
    "        start_time = time.time()\n",
    "        res = func(*args)\n",
    "        end_time = time.time()\n",
    "        print('{} takes {}'.format(func.__name__,end_time-start_time))\n",
    "        return res\n",
    "    return improved_func\n",
    "\n",
    "def load_labels(file):\n",
    "    '''\n",
    "    解码标签文件\n",
    "    '''\n",
    "    with open(file, \"rb\") as f:\n",
    "        data = f.read()  # kp: 任务整理 readlines  readline  and read 的区别\n",
    "    \n",
    "    magic_number, num_samples = struct.unpack(\">ii\", data[:8])  # refer to magic_number.jpg  # struct.unpack refer to https://docs.python.org/3/library/struct.html\n",
    "                                                                # >ii refer to https://docs.python.org/3/library/struct.html\n",
    "    if magic_number != 2049:\n",
    "        print(f\"magic number mismatch {magic_number} != 2049\")\n",
    "        return None\n",
    "\n",
    "    labels = np.array(list(data[8:])) # np.asarray  \n",
    "    return labels\n",
    "\n",
    "def load_images(file):\n",
    "    with open(file, \"rb\") as f: # note rb or r\n",
    "        data = f.read()\n",
    "    \n",
    "    magic_number, num_samples, image_height, image_width = struct.unpack(\">iiii\", data[:16])\n",
    "\n",
    "    if magic_number != 2051:\n",
    "        print(f\"magic number mismatch {magic_number} != 2051\")\n",
    "        return None\n",
    "    \n",
    "    image_data = np.array(list(data[16:]), dtype=np.uint8).reshape(num_samples, -1) # dtype = \"uint8\"\n",
    "\n",
    "    return image_data\n",
    "\n",
    "def one_hot(labels, classes, label_smoothing = 0):\n",
    "    # refer to \n",
    "    n = len(labels) # the number of the samples\n",
    "    alpha = label_smoothing / classes # 公摊系数\n",
    "    output = np.ones((n, classes), dtype= np.float32) * alpha\n",
    "\n",
    "    for row_idx, label in enumerate(labels):\n",
    "        output[row_idx, label] = 1\n",
    "    return output\n",
    "\n",
    "def show_hist(labels, num_classes): # 常用的小工具函数的写法\n",
    "    label_map = {key: 0 for key in range(num_classes)} # 给每一个类都初始化： 数量为0\n",
    "    for label in labels:       # 循环labels，遇到label x  就去label x的keyvalue对里+1\n",
    "        label_map[label] += 1  # 这里相当于是一个一个label item去算\n",
    "    \n",
    "    # label_hist 是一个list, list 的值是 label_map key-value 对儿里的 value\n",
    "    labels_hist = [label_map[key] for key in range(num_classes)]  \n",
    "    pd.DataFrame(labels_hist, columns=[\"label\"]).plot(kind = \"bar\") # api 用法的形象记忆 refer to https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\n",
    "                                                                    # refer to https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html\n",
    "\n",
    "    evaluate(test_loader, W, b1, U, b2)\n",
    "    evaluate(test_loader, W, b1, U, b2)\n",
    "    evaluate(test_loader, W, b1, U, b2)\n",
    "\n",
    "    evaluate(test_loader, W, b1, U, b2)\n",
    "\n",
    "def evaluate(test_loader,model, epoch = None, loss = None):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for test_images, test_labels, _ in test_loader:\n",
    "        probability = softmax(model.inference(test_images))\n",
    "        predict_labels     = probability.argmax(axis=1).reshape(-1, 1)\n",
    "        correct       += (predict_labels == test_labels).sum()\n",
    "    \n",
    "    acc = correct / len(test_dataset)\n",
    "\n",
    "    if (epoch is not None) and (loss is not None):\n",
    "        print(f\"{epoch}. train_Loss: {loss:.3f}, test_Accuracy: {acc:.5f}\") \n",
    "    else:\n",
    "        print(f\"test_Accuracy: {acc:.5f}\") \n",
    "\n",
    "def sigmoid(x):\n",
    "    p0 = x < 0\n",
    "    p1 = ~p0\n",
    "    x = x.copy()\n",
    "\n",
    "    # 如果x的类型是整数，那么会造成丢失精度\n",
    "    x[p0] = np.exp(x[p0]) / (1 + np.exp(x[p0]))\n",
    "    x[p1] = 1 / (1 + np.exp(-x[p1]))\n",
    "    return x\n",
    "\n",
    "def softmax(z):\n",
    "    z = z.copy()\n",
    "    z_max = np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z - z_max)\n",
    "    res = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    return res\n",
    "\n",
    "def lr_schedule_cosine(lr_min, lr_max, per_epochs):\n",
    "    def compute(epoch):\n",
    "        return lr_min + 0.5 * (lr_max - lr_min) * (1 + np.cos(epoch / per_epochs * np.pi))\n",
    "    return compute\n",
    "\n",
    "def estimate_val(predict, gt_labels, classes, loss_func):\n",
    "    plabel = predict.argmax(1)\n",
    "    positive = plabel == gt_labels\n",
    "    total_images = predict.shape[0]\n",
    "    accuracy = sum(positive) / total_images\n",
    "    return accuracy, loss_func(predict, one_hot(gt_labels, classes))\n",
    "\n",
    "\n",
    "#endregion\n",
    "#-------------------------------------------------- 工具函数 ------------------------------------------------------\n",
    "\n",
    "\n",
    "#-------------------------------------------------- 数据集管理 ------------------------------------------------------\n",
    "#region\n",
    "# 创建管理数据和数据加载的类\n",
    "class Dataset:\n",
    "    # 动态的，那么Dataset是个基类，所有动态的继承自Dataset\n",
    "    # 需要实现什么接口？\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class MNIST_Dataset(Dataset):\n",
    "    # 针对mnist数据的解析、加载、预处理(e.g. /255), 加一个全是1的维度etc\n",
    "    def __init__(self, image_file, label_file, use_conv=True):\n",
    "        self.num_classes = 10\n",
    "        self.images = load_images(image_file)\n",
    "        self.labels = load_labels(label_file)\n",
    "\n",
    "\n",
    "        if use_conv:    \n",
    "            # convert 1d to 2d image\n",
    "            self.images = self.images.reshape(-1, 1, 28, 28)\n",
    "\n",
    "\n",
    "        self.images = self.images / 255.0\n",
    "        # self.images = (self.images - np.mean(self.images)) / np.var(self.images)\n",
    "\n",
    "        self.labels_one_hot = one_hot(self.labels, self.num_classes)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" \n",
    "        角色的职责\n",
    "        实现图像加载、归一化/标准化、onehot\n",
    "            为什么要返回one_hot，计算时，使用one_hot比较方便\n",
    "            为什么要返回label，因为做测试的时候，label比较方便\n",
    "            pytorch里面，CELoss使用的不是one_hot。所以以后不需要返回one_hot\n",
    "         \"\"\"\n",
    "        return self.images[index], self.labels[index], self.labels_one_hot[index]\n",
    "\n",
    "    # 获取数据集的长度，个数\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    职责\n",
    "    实例化的时候需要指定dataset，batch_size，shuffle\n",
    "    数据的封装，打包为一个batch\n",
    "    对数据进行打乱\n",
    "    可以通过迭代器来获取一批一批的数据\n",
    "     \"\"\"\n",
    "    def __init__(self, dataset, batch_size, shuffle):\n",
    "        self.dataset = dataset\n",
    "        self.shuffle = shuffle\n",
    "        self.count_data = len(dataset)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        # 实例化一个迭代器对象，将自身作为参数传入进去\n",
    "        return DataLoaderIterator(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" \n",
    "        用以告诉外界，多少次迭代，就算是完成一轮\n",
    "        这里有2种处理方法\n",
    "        1.向上取整\n",
    "        2.整除，向下取整，多余部分丢弃\n",
    "        这里考虑用策略2\n",
    "         \"\"\"\n",
    "        return len(self.dataset) // self.batch_size\n",
    "        \n",
    "class DataLoaderIterator:\n",
    "    \"\"\" \n",
    "    职责：\n",
    "        对打包好的batch一个一个的输出\n",
    "     \"\"\"\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        \n",
    "        # 这里有2中处理策略\n",
    "        # 1.向上取整\n",
    "        # 2.整除，向下取整，多余部分丢弃\n",
    "        # 这里考虑用方法2\n",
    "        self.num_batch_per_epoch = len(dataloader)\n",
    "        \n",
    "        # 定义指针记录当前batch的索引\n",
    "        self.batch_cursor = 0\n",
    "\n",
    "        # 实现一轮数据的打乱和封装获取\n",
    "        # 与其打乱数据，不如打乱索引\n",
    "        self.indexes = list(range(len(dataloader.dataset)))\n",
    "\n",
    "        # 如果需要随机打乱，条件控制由dataloader的shuffle决定\n",
    "        if dataloader.shuffle:\n",
    "            np.random.shuffle(self.indexes)  # inplace e.g. [0,1,2,3 ....59999] --> [2,1,48,23,...0] 完全复现需要你用的是shuffle 还是 np.shuffle\n",
    "\n",
    "    def __next__(self): # 指的是next batch\n",
    "        # 如果到了一轮的边界，即迭代结束，抛出异常 (一上来就做判断)\n",
    "        if self.batch_cursor >= self.num_batch_per_epoch:\n",
    "            # 如果到了边界，抛出StopIteration\n",
    "            raise StopIteration()\n",
    "        \"\"\" \n",
    "        职责：如何一个又一个的数据进行吐出, 每一行是一个数据\n",
    "            b1  image.shape = 784,     label.shape = 1,     label_onehot.shape = 10,\n",
    "            b2  image.shape = 784,     label.shape = 1,     label_onehot.shape = 10,\n",
    "            b3  image.shape = 784,     label.shape = 1,     label_onehot.shape = 10,\n",
    "            ......\n",
    "            n 个 data\n",
    "        \n",
    "        images.shape = n x 784     labels.shape = n x 1        one_hot.shape = n x 10\n",
    "         \"\"\" \n",
    "\n",
    "        batch_data = []\n",
    "        for i in range(self.dataloader.batch_size): # 遍历一个batch里的图片\n",
    "            \"\"\" \n",
    "             拿到图像的索引，这个索引可能是打乱的\n",
    "              \"\"\"\n",
    "            index = self.indexes[self.batch_cursor * self.dataloader.batch_size + i] # 全局idx\n",
    "            # 从dataset中拿到数据 e.g. 一个数据由图像和标签组成\n",
    "            data_item = self.dataloader.dataset[index]\n",
    "\n",
    "            if len(batch_data) == 0:\n",
    "                batch_data = [[] for _ in data_item] # 这里有3个\n",
    "            \n",
    "            # 把data_item中的每一项，分门别类的放到batch_data中\n",
    "            for index, item in enumerate(data_item):\n",
    "                batch_data[index].append(item)\n",
    "\n",
    "\n",
    "        # 遍历完了这个batch里的所有图片，要到下一个batch了\n",
    "        self.batch_cursor += 1\n",
    "\n",
    "        # 当整个batch的数据准备好过后，可以用np.vstack拼接在一起\n",
    "        for index in range(len(batch_data)): # 分别将 img， label， one_hot_label 拼在一起\n",
    "            batch_data[index] = np.stack(batch_data[index]) # !注意防止1维度的消失, 所以用stack\n",
    "\n",
    "        return tuple(batch_data)\n",
    "#endregion\n",
    "#-------------------------------------------------- 数据集管理 ------------------------------------------------------\n",
    "\n",
    "\n",
    "#-------------------------------------------------- 计算流程的管理 ------------------------------------------------------\n",
    "#region\n",
    "\n",
    "\"\"\" \n",
    "思考的时候可以参考下面的流程：\n",
    "    Parameter\n",
    "    Module\n",
    "    Linear\n",
    "    Sigmoid\n",
    "    SoftmaxCrossEntropyloss\n",
    "    Network ---> Sequential\n",
    "    Optimizer\n",
    "    SGD\n",
    "\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "class Module:\n",
    "    \"\"\" \n",
    "        1.可以称之为算子，那么他应该有forward、backward。为了简化代码，可以用__call__实现forward\n",
    "        2.需要实现以个params函数，拿出当前module下的所有【递归，如果有子类里面还有子类包含了参数，也要拿出来】参数实例\n",
    "        3.考虑有些算子，需要感知当前的环境属于训练还是推理，用 train_mode 储存是否为训练状态提供给特定算子用。通过\n",
    "        train方法和eval方法修改 train_mode 的值\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.train_mode = False # 为什么需要呢？一个模块的训练模式可能跟测试模式不一样。比如 模拟考搞点意外\n",
    "\n",
    "    def forward(self, *args):\n",
    "        # forward输入参数可以是多个\n",
    "        raise NotImplementedError() # NotImplementedError : https://blog.csdn.net/grey_csdn/article/details/77074707\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # 假设算子输出只有一个，所以对应的梯度也应该只有一个\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def __call__(self, *args):  # __call__ ref: http://c.biancheng.net/view/2380.html  # 不定长参数\n",
    "        return self.forward(*args)\n",
    "\n",
    "    def train(self):\n",
    "        self.train_mode = True\n",
    "        for m in self.modules(): # 什么意思呢？即 你对任意一个module 开启 train mode 其实是对 它的 sub-module 开启train mode\n",
    "            m.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.train_mode = False\n",
    "        for m in self.modules():\n",
    "            m.eval()\n",
    "    \n",
    "    def modules(self): # 获取一个模块的所有子模块 这里没有递归\n",
    "        ms = []\n",
    "        for attr in self.__dict__: # __dict__  ref: 第一部分 重点理解obj.__dict__即可  https://www.cnblogs.com/starrysky77/p/9102344.html\n",
    "            m = self.__dict__[attr]\n",
    "            if isinstance(m, Module):\n",
    "                ms.append(m)\n",
    "\n",
    "        return ms\n",
    "\n",
    "    def params(self): # 获取一个模块所有的参数（如果有的话） 这里有递归\n",
    "        ps = []\n",
    "        for attr in self.__dict__:\n",
    "            p = self.__dict__[attr]     \n",
    "            if isinstance(p, Parameter): # 先看一下这个p是不是Parameter。kp: 这里用到了递归， 这里是递归的边界  不记得的建议去看一下 python 阶乘 递归\n",
    "                ps.append(p)            \n",
    "        \n",
    "        ms = self.modules()             # 如果不是Parameter的话就直接去找它的子模块（如果有的话）     \n",
    "        for m in ms:         \n",
    "            ps.extend(m.params())       # 对所有的子模块依次获取所有参数\n",
    "\n",
    "        return ps                 \n",
    "\n",
    "    # -----------------小工具方法（非重点）------------------\n",
    "    def info(self, n):\n",
    "        ms = self.modules()  # 拿到所有子模块\n",
    "        name = self.__class__.__name__ # 拿到当前模块的class 名称\n",
    "        output = f\"{name}\\n\" \n",
    "        for m in ms:         # 下面也是递归\n",
    "            output += ('  '*(n+1)) + f\"{m.info(n+1)}\\n\" # 顶头缩进，接着以同样的方式去打印它的子模块\n",
    "\n",
    "        return output[:-1]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.info(0)\n",
    "        \n",
    "class Initializer:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        return self.apply(*args)\n",
    "    \n",
    "class GaussInitializer(Initializer):\n",
    "    def __init__(self, mu, sigma):\n",
    "        super().__init__(\"GaussInit\")\n",
    "        \"\"\" \n",
    "         mu: mean\n",
    "         sigma: the standard deviation. sigma^2 = variance\n",
    "         \"\"\"\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def apply(self, tensor):\n",
    "        tensor[...] = np.random.normal(self.mu, self.sigma, tensor.shape)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class Parameter:\n",
    "    \"\"\" \n",
    "    实例化的时候，传入参数值\n",
    "    封装data、和grad，储存参数值和梯度\n",
    "        grad.shape = forward(x).shape = batch_size x num_output\n",
    "        bias.shape = 1 x num_output\n",
    "     \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.grad = np.zeros_like(data) # del_W 和 W 的形状要是一样大的\n",
    "\n",
    "    # 清空参数中存储的梯度\n",
    "    def zero_grad(self):   # 为什么要清空梯度？ ref: https://blog.csdn.net/weixin_42542536/article/details/104725921\n",
    "        self.grad[...] = 0 # inplace 操作\n",
    "\n",
    "\n",
    "#region Conv and Naive Conv\n",
    "\n",
    "class Kernel():\n",
    "    def __init__(self,arr,stride = 1,padding = 0):\n",
    "        '''\n",
    "        arr :[out_fea,in_fea,ksize,ksize] a.k.a [groups,channels,ksize,ksize]。\n",
    "        下面讨论的都是kernel 的绝对坐标，我们不关心kernel内元素的相对坐标。\n",
    "        arr只有成为class Kenrnel的instance才能拥有以下kernel的所有功能\n",
    "        '''    \n",
    "        self.arr = arr\n",
    "        self.groups,self.channels,self.ksize = self.arr.shape[:3]    # kernel的组数，kernel的通道数，kernel的大小\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.st_offset = self.ksize//2            # (inclusive) img的左上角坐标直接加上start_offset就可以得到kernel在图片上的起点\n",
    "        self.ed_offset = -(self.ksize//2)         # (exclusive) img的右下角直接加上这个end_offset就可以得到kernel在图片上的终点\n",
    "        self.ele_num_each_channel = self.ksize**2 # kernel每个channel的元素个数\n",
    " \n",
    "   \n",
    "    def get_tl(self,center):                      # 通过kernel的中心坐标获取kernel的左上角坐标（wrt img）\n",
    "        cy,cx = center\n",
    "        y,x = cy-self.ksize//2,cx-self.ksize//2   \n",
    "        return y,x\n",
    "    \n",
    "    def get_covered_pixels(self,center,itensor):  # 获取kernel覆盖到的pixels given center and itensor.\n",
    "        y,x = self.get_tl(center) \n",
    "        pixels = itensor[:,:,y:y+self.ksize,x:x+self.ksize]     # 将所有图片，所有通道，kernel覆盖到的像素拿出来\n",
    "        return pixels\n",
    "    \n",
    "    def to_kcol(self):                                          # 将kernel变成kcol\n",
    "        return self.arr.reshape(self.groups,-1)\n",
    "    \n",
    "    def get_output_img_size(self,itensor):\n",
    "        _,_,ih,iw = itensor.shape\n",
    "        oh = (ih-self.ksize + 2*self.padding)//self.stride + 1\n",
    "        ow = (iw-self.ksize + 2*self.padding)//self.stride + 1\n",
    "        return oh,ow\n",
    "\n",
    "class Conv2d(Module):\n",
    "    def __init__(self, in_feature, out_feature, kernel_size, padding=0, stride=1):\n",
    "        super().__init__(\"Conv2d\")\n",
    "        \n",
    "        self.in_feature = in_feature\n",
    "        self.out_feature = out_feature\n",
    "        self.ksz = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        # self.kernel = np.array([  # only used for unit test\n",
    "        #                     [0,0,0],\n",
    "        #                     [1,1,0],\n",
    "        #                     [0,0,0]\n",
    "        #             ])[None][None]\n",
    "        self.knl_arr = np.zeros((self.out_feature, self.in_feature, self.ksz, self.ksz))\n",
    "        self.knl_obj = Kernel(self.knl_arr, stride=self.stride, padding=self.padding) # make kernel_arr to be a real kernel(knl_obj). As soon as we got the knl_obj, we mainly communicate with knl_obj.\n",
    "        self.kcol = self.knl_obj.to_kcol() # (4,9)\n",
    "        \n",
    "        self.weight = Parameter(self.knl_obj.arr)  # Parameter instance\n",
    "        self.bias = Parameter(np.zeros((out_feature)))\n",
    "\n",
    "        initer = GaussInitializer(0, np.sqrt(1 / in_feature))  # np.sqrt(2 / in_feature)\n",
    "        initer.apply(self.weight.data)\n",
    "        pass\n",
    "    \n",
    "    # @cal_time\n",
    "    def forward(self, x):\n",
    "        kcol = self.kcol\n",
    "        knl_obj = self.knl_obj\n",
    "\n",
    "        # 开始构建column（挪kernel)\n",
    "        self.in_shape = x.shape\n",
    "        ib, ic, ih, iw = self.in_shape\n",
    "        self.oh, self.ow = knl_obj.get_output_img_size(x)\n",
    "        self.column = np.zeros((ib, knl_obj.ksize ** 2 * ic, self.oh * self.ow)) # (2,9,676)\n",
    "        self.output = np.zeros((ib, self.out_feature, self.oh ,self.ow)) # (2,4,26,26)\n",
    "\n",
    "        # start to carry pixels from itensor to column\n",
    "        # cy,cx 是kernel的中心\n",
    "        j = 0  # 用来表明是column的第几列\n",
    "        for cy in range(knl_obj.st_offset, ih + knl_obj.ed_offset):\n",
    "            for cx in range(knl_obj.st_offset, iw + knl_obj.ed_offset):\n",
    "                pixels = knl_obj.get_covered_pixels((cy, cx), x)\n",
    "                cols = pixels.reshape(ib, knl_obj.ksize ** 2 * knl_obj.channels, -1)  # 将多张图片的coverer_pixels放到column对应的列里去（涵盖三个通道）\n",
    "                self.column[:, :, None, j] = cols\n",
    "                j += 1\n",
    "\n",
    "        output = kcol @ self.column # (2,4,676)\n",
    "        self.output = output.reshape(ib, knl_obj.groups, self.oh, self.ow) + self.bias.data.reshape(self.out_feature,1,1)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, G): # 这个G已经除以了batchsize\n",
    "        # In im2col, column @ kcol = output\n",
    "        # dcolumn = G @ kcol^T\n",
    "        # dkcol = column^T @ G   G: d_L/d_output\n",
    "        ib, ic, ih, iw = self.in_shape\n",
    "        knl_obj = self.knl_obj\n",
    "        # 1. update part\n",
    "\n",
    "        self.weight.grad = np.sum((G.reshape(-1, self.out_feature, self.oh * self.ow) @ self.column.transpose(0, 2, 1)), axis = 0).reshape(self.knl_arr.shape)\n",
    "        self.bias.grad += np.sum(G, axis=(0, 2, 3))  # 因为G的第一个通道是out_feature,对应的就是有多少组kernel\n",
    "\n",
    "        # 2. pass-back part\n",
    "        self.Gout = np.zeros(self.in_shape)  # because Gout is d_L/d_input\n",
    "\n",
    "        dcolumn = self.kcol.T @ G.reshape(-1, self.out_feature, self.oh * self.ow)  # \"one\" input image --> v output image. If the output channel is v.\n",
    "        # this is the thing to be passed back.\n",
    "        j = 0  # indexing the j-th col in dcolumn starts from 0\n",
    "        for cy in range(knl_obj.st_offset, ih + knl_obj.ed_offset):  # the part you need to fill gradients back into dcolumn\n",
    "            for cx in range(knl_obj.st_offset, iw + knl_obj.ed_offset):\n",
    "                y,x = self.knl_obj.get_tl((cy,cx))\n",
    "                self.Gout[:, :, y:y+self.ksz, x:x+self.ksz] += dcolumn[:,:,j].reshape(ib, self.in_feature, self.ksz, self.ksz)\n",
    "                j+=1\n",
    "\n",
    "        return self.Gout\n",
    "\n",
    "class naive_Conv2d(Module):\n",
    "    def __init__(self,in_feature, out_feature, kernel_size, padding = 0, stride = 1):\n",
    "        super().__init__(\"Conv2d\")\n",
    "        self.in_feature = in_feature\n",
    "        self.out_feature = out_feature\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "\n",
    "        self.kernel = Parameter(np.zeros((out_feature, in_feature, kernel_size, kernel_size)))# 就是这样定义的\n",
    "        # self.kernel = Parameter(\n",
    "        #             np.array([\n",
    "        #                     [0,0,0],\n",
    "        #                     [1,1,0],\n",
    "        #                     [0,0,0] \n",
    "        #             ])[None][None])\n",
    "        self.bias = Parameter(np.zeros((out_feature)))# 每一组kernel 配一个bias\n",
    "        initer = GaussInitializer(0,2/np.sqrt(in_feature))\n",
    "#         initer.apply(self.kernel.data)\n",
    "\n",
    "    # @cal_time \n",
    "    def forward(self,x):\n",
    "        # the input :img and kernel\n",
    "        self.in_shape = x.shape\n",
    "        ib,ic,ih,iw = self.in_shape\n",
    "        self.khalf = self.kernel_size//2\n",
    "        # output\n",
    "        self.oh = (ih-self.kernel_size + 2*self.padding)//self.stride + 1\n",
    "        self.ow = (iw-self.kernel_size + 2*self.padding)//self.stride + 1\n",
    "        self.output = np.zeros((ib,self.out_feature,self.oh,self.ow))\n",
    "    \n",
    "        # column\n",
    "        self.column = np.zeros((ib,self.kernel_size*self.kernel_size*ic ,self.oh*self.ow))\n",
    "        # k_col\n",
    "        self.k_col = self.kernel.data.reshape((self.out_feature,-1))\n",
    "\n",
    "        \n",
    "        for b in range(ib):\n",
    "            for channel in range(ic):\n",
    "                for oy in range(self.oh):# oy ox 指的是输出在输出图像的坐标【跟v1 v2的cy cx不一样】\n",
    "                    for ox in range(self.ow):\n",
    "                        for ky in range(self.kernel_size):\n",
    "                            for kx in range(self.kernel_size):\n",
    "                            # where the pixel value goes in column\n",
    "                                column_y = self.kernel_size**2*channel + ky*self.kernel_size + kx\n",
    "                                column_x = oy*self.ow + ox # ow的格数大小就是kernel横向取了几次\n",
    "                                # where the pixel value comes from img\n",
    "                                iy = oy*self.stride+ky - self.padding\n",
    "                                ix = ox*self.stride+kx - self.padding\n",
    "\n",
    "                                # 如果iy ix超出边界(可能进入了padding地带)，就不处理\n",
    "                                if iy >=0 and iy < ih and ix >= 0 and ix < iw:\n",
    "                                    self.column[b,column_y, column_x] = x[b,channel,iy,ix]\n",
    "            \n",
    "            self.output[b] = (self.k_col @ self.column[b]).reshape(-1,self.oh,self.ow) + self.bias.data.reshape((self.out_feature,1,1))       \n",
    "        return self.output  \n",
    "        \n",
    "    def backward(self,G):# G : G_in : dL/d output(this layer)\n",
    "        ib,ic,ih,iw = self.in_shape # the shape of x  [input of the current layer]\n",
    "        \n",
    "        # 1.update part\n",
    "        # k_col @ column = output\n",
    "        for b in range(ib):\n",
    "            # 首先三维的G[b] 肯定是要reshape成2维。因为G[b]：d output(this layer)，所以shape与output[b]是一样的\n",
    "            # output[b]是[out_feature,oh,ow]\n",
    "            self.kernel.grad += (G[b].reshape(-1,self.oh*self.ow)@self.column[b].T).reshape(self.kernel.data.shape) # column[b].T shape: (oh*ow,kh*kw*channel)\n",
    "        \n",
    "        self.bias.grad += np.sum(G,axis = (0,2,3)) # 因为G的第一个通道是out_feature,对应的就是有多少组kernel\n",
    "        \n",
    "        # 2.pass back part\n",
    "        self.Gout = np.zeros((self.in_shape))\n",
    "\n",
    "        for b in range(ib):\n",
    "            # dcolumn我们这里仅仅作为当前图片的dcolumn\n",
    "            dcolumn = self.k_col.T @ G[b].reshape(self.out_feature,-1) # k_col.T shape: (kw*kh*ic,out_feature)\n",
    "            # dcolumn 和column shape是一样的\n",
    "  \n",
    "            for channel in range(ic):\n",
    "                for oy in range(self.oh):# oy ox 指的是输出在输出图像的坐标【跟v1 v2的cy cx不一样】\n",
    "                    for ox in range(self.ow):\n",
    "                        for ky in range(self.kernel_size):\n",
    "                            for kx in range(self.kernel_size):\n",
    "                            # where the pixel value comes from column\n",
    "                                column_y = self.kernel_size**2*channel + ky*self.kernel_size + kx\n",
    "                                column_x = oy*self.ow + ox # ow的格数大小就是kernel横向取了几次\n",
    "                                # where the pixel value goes to img 可参考 notability 笔记 “输入输出坐标的推导”\n",
    "                                iy = oy*self.stride+ky - self.padding\n",
    "                                ix = ox*self.stride+kx - self.padding\n",
    "\n",
    "                                # 如果iy ix超出边界(可能进入了padding地带)，就不处理\n",
    "                                if iy >=0 and iy < ih and ix >= 0 and ix < iw:\n",
    "                                    self.Gout[b,channel,iy,ix] += dcolumn[column_y, column_x]\n",
    "                                    #上面之所以使用+= 是因为在im2col的时候，一个img像素会搬到column的多个地方\n",
    "                                    #（由于是滑动窗口会重叠），也就是说一个像素会在column不同地方出现，所以回传的时候\n",
    "                                    #有多个地方贡献梯度\n",
    "        \n",
    "        return self.Gout\n",
    "\n",
    "\n",
    "\n",
    "#endregion Conv and Naive Conv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    \"\"\"     \n",
    "    线性算子，线性层\n",
    "     职责:\n",
    "     包含了参数（parameter），包含了运算过程（forward、backward），对于输入的梯度计算，\n",
    "     和对于参数（parameter）的梯度计算\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_feature, out_feature): # 回忆 W  比如: X @ W  = Y   [32x784]@[784x10] = [32x10]\n",
    "        super().__init__(\"Linear\") # 你如果有继承请一定一定一定要写super\n",
    "        self.in_feature = in_feature\n",
    "        self.out_feature = out_feature\n",
    "\n",
    "        # init method\n",
    "        # 试一下在这里加入or改成kaiming init\n",
    "        self.weight = Parameter(np.zeros((in_feature, out_feature)))\n",
    "        self.bias = Parameter(np.zeros((1, out_feature)))\n",
    "\n",
    "        # weight init\n",
    "        initer = GaussInitializer(0, np.sqrt(2 / (in_feature)))\n",
    "        initer.apply(self.weight.data)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 保存x给到backward时使用。 回忆 X @ W = Y    del_W = X^T @ G\n",
    "        self.x = x\n",
    "\n",
    "        return x @ self.weight.data + self.bias.data # 回忆一下，加入weight是[784,10] 那么bias形状是多大[10]\n",
    "\n",
    "    def backward(self, grad):\n",
    "        \"\"\" \n",
    "        回忆矩阵乘法：\n",
    "            X @ W = Y\n",
    "            del_W = X^T @ G\n",
    "            del_X = G   @ W^T\n",
    "\n",
    "         \"\"\"\n",
    "        self.weight.grad += self.x.T @ grad  # 回忆一下所说的更新部分  # 为什么要写成+=????\n",
    "        self.bias.grad   += np.sum(grad, axis = 0, keepdims = True)\n",
    "        \n",
    "        return grad @ self.weight.data.T  # 和反传部分\n",
    "\n",
    "class ReLU(Module):\n",
    "    def __init__(self, inplace = True):\n",
    "        super().__init__(\"ReLU\")\n",
    "        self.inplace = inplace\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.negative_position = x < 0\n",
    "        if not self.inplace:\n",
    "            x = x.copy()\n",
    "        \n",
    "        x[self.negative_position] = 0\n",
    "        return x\n",
    "    \n",
    "    def backward(self, G):\n",
    "        if not self.inplace:\n",
    "            G = G.copy()               # todo\n",
    "        \n",
    "        G[self.negative_position] = 0  # the derivative of relu in <0 is 0\n",
    "        return G                       # the derivative of relu in >=0 is 1. Thus G * 1\n",
    "\n",
    "class PReLU(Module):\n",
    "    def __init__(self, num_feature, inplace=False):\n",
    "        super().__init__(\"PReLU\")\n",
    "        self.inplace = inplace\n",
    "        self.coeff = Parameter(np.zeros((num_feature)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not self.inplace:\n",
    "            x = x.copy()\n",
    "            \n",
    "        for channel in range(x.shape[1]):\n",
    "            view = x[:, channel]\n",
    "            negative_position = view < 0\n",
    "            view[negative_position] *= self.coeff.data[channel]\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Sigmoid\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = self.x = x.copy()\n",
    "        \n",
    "        p0 = self.x < 0\n",
    "        p1 = ~p0\n",
    "        self.x = self.x.copy()\n",
    "\n",
    "        self.out[p0] = np.exp(self.x[p0]) / (1 + np.exp(self.x[p0]))\n",
    "        self.out[p1] = 1 / (1 + np.exp(-self.x[p1]))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad * self.out * (1 - self.out)\n",
    "    \n",
    "\n",
    "class Swish(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Swish\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_save = x.copy()\n",
    "        self.sx = sigmoid(x)\n",
    "        return x * self.sx\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad * (self.sx + self.x_save * self.sx * (1 - self.sx))\n",
    "\n",
    "\n",
    "class Dropout(Module):\n",
    "    def __init__(self, prob_keep=0.5, inplace=True):\n",
    "        super().__init__(\"Dropout\")\n",
    "        self.prob_keep = prob_keep\n",
    "        self.inplace = inplace\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not self.train_mode:\n",
    "            return x\n",
    "        \n",
    "        self.mask = np.random.binomial(size=x.shape, p=1 - self.prob_keep, n=1)\n",
    "        if not self.inplace:\n",
    "            x = x.copy()\n",
    "            \n",
    "        x[self.mask] = 0\n",
    "        x *= 1 / self.prob_keep\n",
    "        return x\n",
    "    \n",
    "    def backward(self, G):\n",
    "        if not self.inplace:\n",
    "            G = G.copy()\n",
    "        G[self.mask] = 0\n",
    "        G *= 1 / self.prob_keep\n",
    "        return G\n",
    "\n",
    "\n",
    "\n",
    "class Maxpool2d(Module):\n",
    "    def __init__(self, kernel_size = 2, stride = 1):\n",
    "        super().__init__(\"Maxpool\")\n",
    "        self.ksize = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x): # not considering padding\n",
    "        self.in_shape = x.shape\n",
    "        ib, ic, ih, iw = x.shape\n",
    "        self.oh = int(np.ceil((ih - self.ksize)/self.stride + 1))  # Using ceil here amounts to inexplictly padding so that size can be exact divided by stride.\n",
    "        self.ow = int(np.ceil((iw - self.ksize)/self.stride + 1))\n",
    "        output = np.zeros((ib,ic,self.oh,self.ow))\n",
    "\n",
    "        for oy in range(self.oh):\n",
    "            for ox in range(self.ow):\n",
    "                output[:,:, oy, ox] = np.max(x[:, :, oy*self.stride: oy*self.stride + self.ksize,\n",
    "                                                     ox*self.stride: ox*self.stride + self.ksize ])\n",
    "\n",
    "            '''\n",
    "            No need to worry about the even or odd value of the size.\n",
    "                let's say we have an x with size (64, 16, 11, 11), given ksz = 2, stride = 2\n",
    "                11 = 2x5 + 1\n",
    "\n",
    "                the first 5 indexing will return \n",
    "                array([[0., 0.],\n",
    "                       [0., 0.]])\n",
    "\n",
    "                But the last one will return \n",
    "                array([[0.],\n",
    "                       [0.]])\n",
    "            '''\n",
    "        return output\n",
    "        pass\n",
    "\n",
    "    def backward(self, G): \n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Flatten(Module):\n",
    "    # flatten仅仅只是将输入一维化，不影响batch大小\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Flatten\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.in_shape = x.shape\n",
    "        self.out = x.reshape(self.in_shape[0], -1)  # 保留batch大小不变\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, G):  # G : dL/dx 所以跟x是一个形状\n",
    "        return G.reshape(self.in_shape)\n",
    "\n",
    "class Dropout(Module):\n",
    "    def __init__(self, pro_keep=0.5, inplace=True):\n",
    "        super().__init__(\"Dropout\")\n",
    "        self.pro_keep = pro_keep\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.train_mode:\n",
    "            return x\n",
    "\n",
    "        self.mask = np.random.binomial(size=x.shape, p=1 - self.pro_keep, n=1)\n",
    "        if not inplace:\n",
    "            x = x.copy()\n",
    "        x[self.mask] = 0  # 压制住每层false的输入神经元\n",
    "        x *= 1 / self.pro_keep  # 需要rescale\n",
    "        return x\n",
    "\n",
    "    def backward(self, G):\n",
    "        if not self.train_mode:\n",
    "            return G\n",
    "\n",
    "        if not inplace:\n",
    "            G = G.copy()\n",
    "\n",
    "        G[self.mask] = 0\n",
    "        G *= 1 / self.pro_keep\n",
    "        return G\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SigmoidCrossEntropyLoss(Module):\n",
    "    def __init__(self, params, weight_decay=1e-5):\n",
    "        super().__init__(\"CrossEntropyLoss\")\n",
    "        self.params = params\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        #return 1 / (1 + np.exp(-x))\n",
    "        p0 = x < 0\n",
    "        p1 = ~p0\n",
    "        x = x.copy()\n",
    "        x[p0] = np.exp(x[p0]) / (1 + np.exp(x[p0]))\n",
    "        x[p1] = 1 / (1 + np.exp(-x[p1]))\n",
    "        return x\n",
    "    \n",
    "    def decay_loss(self):\n",
    "        loss = 0\n",
    "        for p in self.params:\n",
    "            loss += np.sqrt(np.sum(p.data ** 2)) / (2 * p.data.size) * self.weight_decay\n",
    "        return loss\n",
    "    \n",
    "    def decay_backward(self):\n",
    "        for p in self.params:\n",
    "            eps = 1e-8\n",
    "            p.grad += 1 / (2 * np.sqrt(np.sum(p.data ** 2)) + eps) / (2 * p.data.size) * self.weight_decay * 2 * p.data\n",
    "\n",
    "    def forward(self, x, label_onehot):\n",
    "        eps = 1e-6\n",
    "        self.label_onehot = label_onehot\n",
    "        self.predict = self.sigmoid(x)\n",
    "        self.predict = np.clip(self.predict, a_max=1-eps, a_min=eps)  # 裁切\n",
    "        self.batch_size = self.predict.shape[0]\n",
    "        return -np.sum(label_onehot * np.log(self.predict) + (1 - label_onehot) * \n",
    "                        np.log(1 - self.predict)) / self.batch_size + self.decay_loss()\n",
    "    \n",
    "    def backward(self):\n",
    "        self.decay_backward()\n",
    "        return (self.predict - self.label_onehot) / self.batch_size\n",
    "    \n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropyLoss(Module): # 一般的我们会把softmax 跟 loss合起来\n",
    "    def __init__(self):\n",
    "        super().__init__(\"SoftmaxCrossEntropy\")\n",
    "    \n",
    "\n",
    "    def forward(self, x, label_onehot):\n",
    "        eps = 1e-6 #! 跟同学对比的时候，你们大家是不是都做了剪裁\n",
    "        self.label_onehot = label_onehot\n",
    "        self.predict = softmax(x)\n",
    "        self.predict = np.clip(self.predict, a_max=1-eps, a_min=eps)  # 裁切\n",
    "        self.batch_size = self.predict.shape[0]\n",
    "        return -np.sum(label_onehot * np.log(self.predict)) / self.batch_size #! 搞清楚哪里要除以，哪里不用，不要这里除了，那里又除多一次\n",
    "\n",
    "\n",
    "    def backward(self,grad = 1):\n",
    "        return grad * (self.predict - self.label_onehot) / self.batch_size\n",
    "\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, *items): # 初始化的时候放入一些模块\n",
    "        super().__init__(\"Sequential\")\n",
    "        self.items = items\n",
    "\n",
    "    def modules(self): # 只是返回最浅层即可\n",
    "        \"\"\" \n",
    "        覆盖基类的modules方法，直接返回items即可\n",
    "         \"\"\"\n",
    "        return self.items\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 按照顺序执行items即可\n",
    "        for m in self.items:\n",
    "            x = m(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # 按照反向顺序，执行items中模块的backward\n",
    "        for item in self.items[::-1]: # kp：逆序\n",
    "            grad = item.backward(grad)\n",
    "            \n",
    "        return grad\n",
    "\n",
    "class Model(Module):\n",
    "    def __init__(self, use_conv = True): # 可以把Network理解成一个大矩阵。这里num_input就是num_feature 784\n",
    "        super().__init__(\"Model\")\n",
    "\n",
    "        # use_conv\n",
    "        if use_conv:\n",
    "            self.backbone = Sequential(\n",
    "                # naive_Conv2d(in_feature = 1, out_feature = 5, kernel_size = 3),\n",
    "                Conv2d(in_feature = 1, out_feature = 5, kernel_size = 3),\n",
    "                ReLU(),\n",
    "                Flatten(),\n",
    "                Linear(in_feature = 3380, out_feature = 10)\n",
    "            )\n",
    "\n",
    "        # BP\n",
    "        else:\n",
    "            self.backbone = Sequential(\n",
    "                Linear(784, 50),\n",
    "                ReLU(),\n",
    "                Linear(50, 10)\n",
    "            )\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return self.backbone.backward(grad)\n",
    "\n",
    "    def save(self, f):\n",
    "        param_list = []\n",
    "        for p in self.params():\n",
    "            data = p.data\n",
    "            grad = p.grad\n",
    "            param_list.append((data,grad))\n",
    "\n",
    "\n",
    "        pkl.dump(param_list,open(f\"{f}\",\"wb\"))\n",
    "        print(f\"the model has been saved.\")\n",
    "        \n",
    "            \n",
    "    def load(self, f):\n",
    "        param_list = pkl.load(open(f\"{f}\",\"rb\"))\n",
    "        param_module_list = self.params()\n",
    "\n",
    "        for idx ,p in enumerate(param_list): # \n",
    "            param_module_list[idx].data = p[0] # e.g. (784, 512) \n",
    "            param_module_list[idx].grad = p[1] # e.g. (784, 512)\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, name, model, lr):\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.params = model.params()\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        # 清空所有参数中的梯度\n",
    "        # 如果需要累计梯度，可以自行控制\n",
    "        for param in self.params:\n",
    "            param.zero_grad() # 调用的是class Parameter 的 zero_grad()\n",
    "    \n",
    "    def step(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def set_lr(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        super().__init__(\"SGD\", model, lr)\n",
    "    \n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.data -= self.lr * param.grad\n",
    "\n",
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3, momentum=0.9):\n",
    "        super().__init__(\"SGDMomentum\", model, lr)\n",
    "        self.momentum = momentum\n",
    "\n",
    "        for param in self.params:\n",
    "            param.v = 0\n",
    "\n",
    "    # 移动平均\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.v = self.momentum * param.v - self.lr * param.grad\n",
    "            param.data += param.v\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3, beta1=0.9, beta2=0.999, l2_regularization = 0):\n",
    "        super().__init__(\"Adam\", model, lr)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.l2_regularization = l2_regularization\n",
    "        self.t = 0\n",
    "        \n",
    "        for param in self.params:\n",
    "            param.m = 0\n",
    "            param.v = 0\n",
    "            \n",
    "    # 指数移动平均\n",
    "    def step(self):\n",
    "        eps = 1e-8\n",
    "        self.t += 1\n",
    "        for param in self.params:\n",
    "            g = param.grad\n",
    "            param.m = self.beta1 * param.m + (1 - self.beta1) * g\n",
    "            param.v = self.beta2 * param.v + (1 - self.beta2) * g ** 2\n",
    "            mt_ = param.m / (1 - self.beta1 ** self.t)\n",
    "            vt_ = param.v / (1 - self.beta2 ** self.t)\n",
    "            param.data -= self.lr * mt_ / (np.sqrt(vt_) + eps) + self.l2_regularization * param.data\n",
    "\n",
    "#endregion\n",
    "#-------------------------------------------------- 计算流程的管理 ------------------------------------------------------\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "'''\n",
    "提取参数的例子：\n",
    "net.layer[4].blobs[0].data\n",
    "'''\n",
    "import mtcnn.caffe_pb2 as pb\n",
    "\n",
    "net = pb.NetParameter()\n",
    "\n",
    "with open(\"mtcnn/det1.caffemodel\",'rb') as f: # .caffemodel 存放的是二进制模型参数\n",
    "    net.ParseFromString(f.read())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "net.layer[4].blobs[0].data[:10]#仅仅打印前面10项做示范\n",
    "\n",
    "lista = [net.layer[i].name for i in range(len(net.layer))]\n",
    "lista\n",
    "np.array(net.layer[4].blobs[0].data).reshape(net.layer[4].blobs[0].shape.dim)\n",
    "net.layer[0] #可以索引，一般就可以for 循环直接迭代取出   for layer in net.layer\n",
    "name2layer_map = {layer.name : layer for layer in net.layer}\n",
    "net.layer[5].blobs[0].data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[-0.6254063248634338, -0.21168090403079987, 1.011683464050293, 0.44112059473991394, -1.0347496271133423, 0.3215140402317047, -0.2978908121585846, 0.2518868148326874, -0.8827362060546875, -1.2783164978027344]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 经过实验我们发现"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "\n",
    "# net.layer[4] #--- conv1\n",
    "# net.layer[5] #--- PRelu\n",
    "# net.layer[6] #--- pooling 以此类推\n",
    "# net.layer[12] #----\n",
    "# net.layer[12] # name: \"conv4-1\"\n",
    "\n",
    "conv4_2 = net.layer[14].blobs[0].data # name: \"conv4-2\"  # 有多少个out_feature 就有多少个bias\n",
    "conv4_2 = np.array(conv4_2).reshape(32,4,1,1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Homework"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# 读取图片\n",
    "import cv2\n",
    "img = cv2.imread('my.jpg')\n",
    "img = cv2.resize(img,dsize=(12,12))\n",
    "plt.imshow(img[:,:,::-1])\n",
    "img.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(12, 12, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 37
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "7383003b210fdacca9bf7683d9d1d561f4a72c77adad40daede406a89507eb7d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}